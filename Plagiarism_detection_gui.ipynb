{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fa7665c-072d-4bf1-b764-dc796796226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from clang.cindex import Config\n",
    "import clang.cindex\n",
    "\n",
    "def find_libclang():\n",
    "    # Try multiple possible locations for libclang.dll\n",
    "    possible_paths = [\n",
    "        # Anaconda path\n",
    "        Path(sys.prefix) / 'Library' / 'bin' / 'libclang.dll',\n",
    "        Path(sys.prefix) / 'clang' / 'native' / 'libclang.dll',\n",
    "        # pip installed path\n",
    "        Path(sys.prefix) / 'Lib' / 'site-packages' / 'clang' / 'native' / 'libclang.dll',\n",
    "        # System paths\n",
    "        Path('C:/Program Files/LLVM/bin/libclang.dll'),\n",
    "    ]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        if path.exists():\n",
    "            return str(path)\n",
    "    return None\n",
    "\n",
    "# Find and configure libclang.dll\n",
    "libclang_path = find_libclang()\n",
    "if libclang_path:\n",
    "    Config.set_library_file(libclang_path)\n",
    "else:\n",
    "    print(\"Warning: Could not find libclang.dll in any of the expected locations\")\n",
    "    print(\"Please install LLVM and make sure it's in your PATH\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Import required libraries\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "import logging\n",
    "from typing import Dict, List, Set, Tuple, Optional\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "import concurrent.futures\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "# For AST parsing\n",
    "from clang.cindex import Index, CursorKind, TokenKind, TranslationUnit\n",
    "import clang.cindex\n",
    "\n",
    "# For similarity metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from zss import simple_distance, Node\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('plagiarism_detector.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "class TokenProcessor:\n",
    "    # Define token significance levels\n",
    "    TOKEN_SIGNIFICANCE = {\n",
    "        TokenKind.KEYWORD: 1.0,      # High significance\n",
    "        TokenKind.IDENTIFIER: 0.8,\n",
    "        TokenKind.LITERAL: 0.7,\n",
    "        TokenKind.PUNCTUATION: 0.2,   # Low significance\n",
    "        TokenKind.COMMENT: 0.1       # Very low significance\n",
    "    }\n",
    "    \n",
    "    # Tokens to filter out completely\n",
    "    IGNORED_TOKENS = {';', '{', '}', '(', ')', ','}\n",
    "    \n",
    "    @classmethod\n",
    "    def filter_tokens(cls, tokens: List[Tuple[TokenKind, str]]) -> List[Tuple[TokenKind, str, float]]:\n",
    "        \"\"\"Filter and weight tokens based on their significance.\"\"\"\n",
    "        filtered_tokens = []\n",
    "        \n",
    "        for token_kind, token_text in tokens:\n",
    "            # Skip ignored tokens\n",
    "            if token_text in cls.IGNORED_TOKENS:\n",
    "                continue\n",
    "                \n",
    "            # Get token significance\n",
    "            significance = cls.TOKEN_SIGNIFICANCE.get(token_kind, 0.5)\n",
    "            \n",
    "            # Add to filtered list with significance weight\n",
    "            filtered_tokens.append((token_kind, token_text, significance))\n",
    "        \n",
    "        return filtered_tokens\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_identifiers(tokens: List[Tuple[TokenKind, str, float]]) -> List[Tuple[TokenKind, str, float]]:\n",
    "        \"\"\"Normalize identifiers while preserving their role.\"\"\"\n",
    "        id_map = {}\n",
    "        normalized_tokens = []\n",
    "        \n",
    "        for token_kind, token_text, significance in tokens:\n",
    "            if token_kind == TokenKind.IDENTIFIER:\n",
    "                if token_text not in id_map:\n",
    "                    id_map[token_text] = f'ID_{len(id_map)}'\n",
    "                normalized_tokens.append((token_kind, id_map[token_text], significance))\n",
    "            else:\n",
    "                normalized_tokens.append((token_kind, token_text, significance))\n",
    "        \n",
    "        return normalized_tokens\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_cpp_patterns(tokens):\n",
    "        \"\"\"Extract C++ specific patterns from tokens.\"\"\"\n",
    "        patterns = {\n",
    "            'memory_management': [],\n",
    "            'template_usage': [],\n",
    "            'stl_usage': [],\n",
    "            'pointer_usage': []\n",
    "        }\n",
    "        \n",
    "        # Look for memory management patterns\n",
    "        for i, (kind, text, _) in enumerate(tokens):\n",
    "            if text in ['new', 'delete', 'malloc', 'free', 'alloc']:\n",
    "                patterns['memory_management'].append((i, text))\n",
    "            elif text in ['template', 'typename', 'class'] and kind == TokenKind.KEYWORD:\n",
    "                patterns['template_usage'].append((i, text))\n",
    "            elif text in ['vector', 'map', 'set', 'list', 'queue', 'stack', 'array', 'deque']:\n",
    "                patterns['stl_usage'].append((i, text))\n",
    "            elif text in ['*', '&', '->', 'nullptr', 'NULL']:\n",
    "                patterns['pointer_usage'].append((i, text))\n",
    "        \n",
    "        return patterns\n",
    "\n",
    "class ASTAnalyzer:\n",
    "    def __init__(self, file_path: str):\n",
    "        self.file_path = file_path\n",
    "        self.index = Index.create()\n",
    "        self.ast_graph = nx.DiGraph()\n",
    "        self.subtree_hashes = {}\n",
    "        self.cfg_graph = nx.DiGraph()  # Control Flow Graph\n",
    "        \n",
    "        try:\n",
    "            self.tu = self.index.parse(\n",
    "                file_path, \n",
    "                args=['-x', 'c++', '-std=c++14'],\n",
    "                options=TranslationUnit.PARSE_DETAILED_PROCESSING_RECORD\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to parse {file_path}: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _hash_subtree(self, node, memo=None) -> int:\n",
    "        \"\"\"Compute a hash for the subtree rooted at node.\"\"\"\n",
    "        if memo is None:\n",
    "            memo = {}\n",
    "            \n",
    "        if node in memo:\n",
    "            return memo[node]\n",
    "        \n",
    "        # Create a string representation of the node\n",
    "        node_str = f\"{node.kind}_{node.spelling}\"\n",
    "        \n",
    "        # Recursively hash children\n",
    "        child_hashes = []\n",
    "        for child in node.get_children():\n",
    "            child_hash = self._hash_subtree(child, memo)\n",
    "            child_hashes.append(child_hash)\n",
    "        \n",
    "        # Combine node and child hashes\n",
    "        combined = node_str + '_'.join(map(str, sorted(child_hashes)))\n",
    "        result = hash(combined)\n",
    "        \n",
    "        memo[node] = result\n",
    "        return result\n",
    "    \n",
    "    def _to_zss_tree(self, node) -> Node:\n",
    "        \"\"\"Convert AST node to format suitable for Zhang-Shasha algorithm.\"\"\"\n",
    "        children = [self._to_zss_tree(child) for child in node.get_children()]\n",
    "        return Node(f\"{node.kind}_{node.spelling}\", children)\n",
    "    \n",
    "    def _process_node(self, node, parent_id=None, depth=0) -> str:\n",
    "        \"\"\"Process an AST node and build the graph structure.\"\"\"\n",
    "        node_id = f\"{node.kind}_{depth}_{len(self.ast_graph)}\"\n",
    "        \n",
    "        # Store node information\n",
    "        self.ast_graph.add_node(\n",
    "            node_id,\n",
    "            kind=node.kind.name,\n",
    "            spelling=node.spelling,\n",
    "            type=str(node.type.spelling if node.type else \"\"),\n",
    "            depth=depth,\n",
    "            hash=self._hash_subtree(node)\n",
    "        )\n",
    "        \n",
    "        if parent_id:\n",
    "            self.ast_graph.add_edge(parent_id, node_id)\n",
    "        \n",
    "        # Process children\n",
    "        for child in node.get_children():\n",
    "            self._process_node(child, node_id, depth + 1)\n",
    "        \n",
    "        return node_id\n",
    "    \n",
    "    def _build_control_flow_graph(self, node, parent_block=None):\n",
    "        \"\"\"Build a control flow graph from AST.\"\"\"\n",
    "        if node.kind in [CursorKind.FUNCTION_DECL, CursorKind.CXX_METHOD]:\n",
    "            # Create entry block for function\n",
    "            entry_block = f\"entry_{node.spelling}\"\n",
    "            self.cfg_graph.add_node(entry_block, type=\"entry\", ast_node=node)\n",
    "            \n",
    "            current_block = entry_block\n",
    "            \n",
    "            # Process function body\n",
    "            for child in node.get_children():\n",
    "                if child.kind == CursorKind.COMPOUND_STMT:\n",
    "                    self._process_compound_stmt(child, current_block)\n",
    "        \n",
    "    def _process_compound_stmt(self, node, parent_block):\n",
    "        \"\"\"Process a compound statement for CFG.\"\"\"\n",
    "        current_block = parent_block\n",
    "        \n",
    "        for child in node.get_children():\n",
    "            if child.kind == CursorKind.IF_STMT:\n",
    "                # Create condition block\n",
    "                cond_block = f\"if_cond_{len(self.cfg_graph)}\"\n",
    "                self.cfg_graph.add_node(cond_block, type=\"condition\", ast_node=child)\n",
    "                self.cfg_graph.add_edge(current_block, cond_block)\n",
    "                \n",
    "                # Process then and else branches\n",
    "                then_block = None\n",
    "                else_block = None\n",
    "                \n",
    "                for branch in child.get_children():\n",
    "                    if branch.kind == CursorKind.COMPOUND_STMT:\n",
    "                        if not then_block:\n",
    "                            then_block = f\"then_{len(self.cfg_graph)}\"\n",
    "                            self.cfg_graph.add_node(then_block, type=\"then\", ast_node=branch)\n",
    "                            self.cfg_graph.add_edge(cond_block, then_block)\n",
    "                            self._process_compound_stmt(branch, then_block)\n",
    "                        else:\n",
    "                            else_block = f\"else_{len(self.cfg_graph)}\"\n",
    "                            self.cfg_graph.add_node(else_block, type=\"else\", ast_node=branch)\n",
    "                            self.cfg_graph.add_edge(cond_block, else_block)\n",
    "                            self._process_compound_stmt(branch, else_block)\n",
    "                \n",
    "                # Create merge block\n",
    "                merge_block = f\"merge_{len(self.cfg_graph)}\"\n",
    "                self.cfg_graph.add_node(merge_block, type=\"merge\")\n",
    "                \n",
    "                if then_block:\n",
    "                    self.cfg_graph.add_edge(then_block, merge_block)\n",
    "                if else_block:\n",
    "                    self.cfg_graph.add_edge(else_block, merge_block)\n",
    "                else:\n",
    "                    self.cfg_graph.add_edge(cond_block, merge_block)\n",
    "                \n",
    "                current_block = merge_block\n",
    "                \n",
    "            elif child.kind == CursorKind.WHILE_STMT or child.kind == CursorKind.FOR_STMT:\n",
    "                # Create loop header block\n",
    "                loop_header = f\"loop_header_{len(self.cfg_graph)}\"\n",
    "                self.cfg_graph.add_node(loop_header, type=\"loop_header\", ast_node=child)\n",
    "                self.cfg_graph.add_edge(current_block, loop_header)\n",
    "                \n",
    "                # Create loop body block\n",
    "                loop_body = f\"loop_body_{len(self.cfg_graph)}\"\n",
    "                self.cfg_graph.add_node(loop_body, type=\"loop_body\")\n",
    "                self.cfg_graph.add_edge(loop_header, loop_body)\n",
    "                \n",
    "                # Process loop body\n",
    "                for branch in child.get_children():\n",
    "                    if branch.kind == CursorKind.COMPOUND_STMT:\n",
    "                        self._process_compound_stmt(branch, loop_body)\n",
    "                \n",
    "                # Loop back to header\n",
    "                self.cfg_graph.add_edge(loop_body, loop_header)\n",
    "                \n",
    "                # Create exit block\n",
    "                loop_exit = f\"loop_exit_{len(self.cfg_graph)}\"\n",
    "                self.cfg_graph.add_node(loop_exit, type=\"loop_exit\")\n",
    "                self.cfg_graph.add_edge(loop_header, loop_exit)\n",
    "                \n",
    "                current_block = loop_exit\n",
    "            else:\n",
    "                # Regular statement\n",
    "                stmt_block = f\"stmt_{len(self.cfg_graph)}\"\n",
    "                self.cfg_graph.add_node(stmt_block, type=\"statement\", ast_node=child)\n",
    "                self.cfg_graph.add_edge(current_block, stmt_block)\n",
    "                current_block = stmt_block\n",
    "    \n",
    "    def _identify_loops(self, node, loops=None):\n",
    "        \"\"\"Identify loops in the AST.\"\"\"\n",
    "        if loops is None:\n",
    "            loops = []\n",
    "            \n",
    "        if node.kind in [CursorKind.WHILE_STMT, CursorKind.FOR_STMT, CursorKind.DO_STMT]:\n",
    "            loops.append(node)\n",
    "            \n",
    "        for child in node.get_children():\n",
    "            self._identify_loops(child, loops)\n",
    "            \n",
    "        return loops\n",
    "    \n",
    "    def _calculate_cyclomatic_complexity(self):\n",
    "        \"\"\"Calculate cyclomatic complexity from the CFG.\"\"\"\n",
    "        if not self.cfg_graph:\n",
    "            return 1  # Default for empty graph\n",
    "        \n",
    "        # M = E - N + 2P where E is edges, N is nodes, P is connected components\n",
    "        edges = len(self.cfg_graph.edges())\n",
    "        nodes = len(self.cfg_graph.nodes())\n",
    "        components = nx.number_connected_components(self.cfg_graph.to_undirected())\n",
    "        \n",
    "        return edges - nodes + 2 * components\n",
    "    \n",
    "    def _estimate_time_complexity(self, loops):\n",
    "        \"\"\"Estimate time complexity based on loop nesting.\"\"\"\n",
    "        if not loops:\n",
    "            return \"O(1)\"\n",
    "            \n",
    "        # Check for nested loops\n",
    "        max_nesting = 0\n",
    "        current_nesting = 0\n",
    "        visited = set()\n",
    "        \n",
    "        for loop in loops:\n",
    "            if loop in visited:\n",
    "                continue\n",
    "                \n",
    "            current_nesting = 1\n",
    "            parent = loop.semantic_parent\n",
    "            while parent and parent.kind != CursorKind.TRANSLATION_UNIT:\n",
    "                if parent.kind in [CursorKind.WHILE_STMT, CursorKind.FOR_STMT, CursorKind.DO_STMT]:\n",
    "                    current_nesting += 1\n",
    "                parent = parent.semantic_parent\n",
    "                \n",
    "            max_nesting = max(max_nesting, current_nesting)\n",
    "            visited.add(loop)\n",
    "            \n",
    "        if max_nesting == 1:\n",
    "            return \"O(n)\"\n",
    "        elif max_nesting == 2:\n",
    "            return \"O(n²)\"\n",
    "        elif max_nesting == 3:\n",
    "            return \"O(n³)\"\n",
    "        elif max_nesting > 3:\n",
    "            return f\"O(n^{max_nesting})\"\n",
    "        else:\n",
    "            return \"O(1)\"\n",
    "\n",
    "    def analyze(self) -> Dict:\n",
    "        \"\"\"Perform complete AST analysis with subtree matching.\"\"\"\n",
    "        try:\n",
    "            root_id = self._process_node(self.tu.cursor)\n",
    "            zss_tree = self._to_zss_tree(self.tu.cursor)\n",
    "            \n",
    "            # Build control flow graph\n",
    "            self._build_control_flow_graph(self.tu.cursor)\n",
    "            \n",
    "            # Identify loops for complexity analysis\n",
    "            loops = self._identify_loops(self.tu.cursor)\n",
    "            \n",
    "            # Calculate structural metrics\n",
    "            max_depth = max(d['depth'] for _, d in self.ast_graph.nodes(data=True))\n",
    "            node_types = {d['kind'] for _, d in self.ast_graph.nodes(data=True)}\n",
    "            subtree_hashes = {n: d['hash'] for n, d in self.ast_graph.nodes(data=True)}\n",
    "            \n",
    "            # Calculate complexity metrics\n",
    "            cyclomatic_complexity = self._calculate_cyclomatic_complexity()\n",
    "            time_complexity = self._estimate_time_complexity(loops)\n",
    "            \n",
    "            # Safely calculate max nesting\n",
    "            max_nesting = 1  # Default value\n",
    "            if loops:\n",
    "                nesting_values = [1]  # Start with default\n",
    "                for node in loops:\n",
    "                    try:\n",
    "                        # Safely get token count if available\n",
    "                        if node.get_definition() and hasattr(node.get_definition().get_tokens(), 'count'):\n",
    "                            nesting_values.append(node.get_definition().get_tokens().count)\n",
    "                    except:\n",
    "                        pass  # Skip if we can't get tokens\n",
    "                max_nesting = max(nesting_values)\n",
    "            \n",
    "            return {\n",
    "                'ast_depth': max_depth,\n",
    "                'node_count': len(self.ast_graph),\n",
    "                'node_types': list(node_types),\n",
    "                'subtree_hashes': subtree_hashes,\n",
    "                'zss_tree': zss_tree,\n",
    "                'graph_structure': {\n",
    "                    'edges': len(self.ast_graph.edges()),\n",
    "                    'avg_branching': len(self.ast_graph.edges()) / max(1, len(self.ast_graph.nodes())),\n",
    "                    'leaf_nodes': sum(1 for n in self.ast_graph.nodes() \n",
    "                                    if self.ast_graph.out_degree(n) == 0)\n",
    "                },\n",
    "                'control_flow': {\n",
    "                    'cfg_nodes': len(self.cfg_graph.nodes()),\n",
    "                    'cfg_edges': len(self.cfg_graph.edges()),\n",
    "                    'cyclomatic_complexity': cyclomatic_complexity\n",
    "                },\n",
    "                'complexity': {\n",
    "                    'time_complexity': time_complexity,\n",
    "                    'loop_count': len(loops),\n",
    "                    'max_nesting': max_nesting\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in AST analysis for {self.file_path}: {str(e)}\")\n",
    "            raise\n",
    "     \n",
    "        \n",
    "class ScalableSimilarityAnalyzer:\n",
    "    def __init__(self, num_perm: int = 128, threshold: float = 0.8):\n",
    "        self.num_perm = num_perm\n",
    "        self.threshold = threshold\n",
    "        self.lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)\n",
    "        self.minhashes = {}\n",
    "    \n",
    "    def _create_minhash(self, tokens: List[str]) -> MinHash:\n",
    "        \"\"\"Create MinHash signature for a sequence of tokens.\"\"\"\n",
    "        m = MinHash(num_perm=self.num_perm)\n",
    "        for token in tokens:\n",
    "            m.update(token.encode('utf-8'))\n",
    "        return m\n",
    "    \n",
    "    def add_submission(self, file_id: str, tokens: List[str]):\n",
    "        \"\"\"Add a submission to the LSH index.\"\"\"\n",
    "        minhash = self._create_minhash(tokens)\n",
    "        self.minhashes[file_id] = minhash\n",
    "        self.lsh.insert(file_id, minhash)\n",
    "    \n",
    "    def find_similar(self, file_id: str) -> Set[str]:\n",
    "        \"\"\"Find similar submissions using LSH.\"\"\"\n",
    "        return self.lsh.query(self.minhashes[file_id])\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_tree_distance(tree1: Node, tree2: Node) -> float:\n",
    "        \"\"\"Compute normalized tree edit distance with optimizations.\"\"\"\n",
    "        # Use memoization for better performance\n",
    "        memo = {}\n",
    "        \n",
    "        def tree_size(node):\n",
    "            if hasattr(node, '_size'):\n",
    "                return node._size\n",
    "            size = 1 + sum(tree_size(child) for child in node.children)\n",
    "            node._size = size\n",
    "            return size\n",
    "        \n",
    "        # Pre-compute sizes\n",
    "        size1 = tree_size(tree1)\n",
    "        size2 = tree_size(tree2)\n",
    "        \n",
    "        # If sizes are very different, we can short-circuit\n",
    "        if abs(size1 - size2) / max(size1, size2) > 0.5:\n",
    "            return 0.0\n",
    "        \n",
    "        # Compute actual distance\n",
    "        distance = simple_distance(tree1, tree2)\n",
    "        max_size = max(size1, size2)\n",
    "        \n",
    "        # Normalize\n",
    "        return 1 - (distance / max_size)\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_cfg_similarity(cfg1: nx.DiGraph, cfg2: nx.DiGraph) -> float:\n",
    "        \"\"\"Compute similarity between control flow graphs.\"\"\"\n",
    "        if not cfg1.nodes() or not cfg2.nodes():\n",
    "            return 0.0\n",
    "            \n",
    "        # Compare basic graph properties\n",
    "        nodes_sim = min(len(cfg1), len(cfg2)) / max(len(cfg1), len(cfg2))\n",
    "        edges_sim = min(len(cfg1.edges()), len(cfg2.edges())) / max(len(cfg1.edges()), len(cfg2.edges())) if max(len(cfg1.edges()), len(cfg2.edges())) > 0 else 1.0\n",
    "        \n",
    "        # Compare node types distribution\n",
    "        types1 = {}\n",
    "        types2 = {}\n",
    "        \n",
    "        for _, data in cfg1.nodes(data=True):\n",
    "            node_type = data.get('type', 'unknown')\n",
    "            types1[node_type] = types1.get(node_type, 0) + 1\n",
    "            \n",
    "        for _, data in cfg2.nodes(data=True):\n",
    "            node_type = data.get('type', 'unknown')\n",
    "            types2[node_type] = types2.get(node_type, 0) + 1\n",
    "            \n",
    "        # Jaccard similarity of node types\n",
    "        all_types = set(types1.keys()) | set(types2.keys())\n",
    "        type_sim = sum(min(types1.get(t, 0), types2.get(t, 0)) for t in all_types) / sum(max(types1.get(t, 0), types2.get(t, 0)) for t in all_types) if all_types else 0\n",
    "        \n",
    "        # Weighted combination\n",
    "        return 0.3 * nodes_sim + 0.3 * edges_sim + 0.4 * type_sim\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_complexity_similarity(comp1: Dict, comp2: Dict) -> float:\n",
    "        \"\"\"Compare algorithmic complexity signatures.\"\"\"\n",
    "        # Time complexity comparison\n",
    "        time_match = comp1['time_complexity'] == comp2['time_complexity']\n",
    "        \n",
    "        # Loop count similarity\n",
    "        loop_sim = 1 - abs(comp1['loop_count'] - comp2['loop_count']) / max(comp1['loop_count'], comp2['loop_count']) if max(comp1['loop_count'], comp2['loop_count']) > 0 else 1.0\n",
    "        \n",
    "        # Nesting depth similarity\n",
    "        nesting_sim = 1 - abs(comp1['max_nesting'] - comp2['max_nesting']) / max(comp1['max_nesting'], comp2['max_nesting']) if max(comp1['max_nesting'], comp2['max_nesting']) > 0 else 1.0\n",
    "        \n",
    "        # Weighted combination\n",
    "        return 0.5 * (1 if time_match else 0) + 0.25 * loop_sim + 0.25 * nesting_sim\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_weighted_similarity(submission1: Dict, submission2: Dict) -> Dict:\n",
    "        \"\"\"Compute comprehensive similarity with weighted metrics.\"\"\"\n",
    "        # Tree edit distance similarity\n",
    "        tree_sim = ScalableSimilarityAnalyzer.compute_tree_distance(\n",
    "            submission1['ast_features']['zss_tree'],\n",
    "            submission2['ast_features']['zss_tree']\n",
    "        )\n",
    "        \n",
    "        # Subtree hash similarity\n",
    "        hashes1 = set(submission1['ast_features']['subtree_hashes'].values())\n",
    "        hashes2 = set(submission2['ast_features']['subtree_hashes'].values())\n",
    "        hash_sim = len(hashes1.intersection(hashes2)) / len(hashes1.union(hashes2))\n",
    "        \n",
    "        # Structure similarity\n",
    "        struct1 = submission1['ast_features']['graph_structure']\n",
    "        struct2 = submission2['ast_features']['graph_structure']\n",
    "        struct_sim = 1 - abs(struct1['avg_branching'] - struct2['avg_branching']) / \\\n",
    "                     max(struct1['avg_branching'], struct2['avg_branching'])\n",
    "        \n",
    "        # Control flow similarity\n",
    "        cfg_sim = ScalableSimilarityAnalyzer.compute_cfg_similarity(\n",
    "            submission1['ast_features'].get('control_flow', {}).get('cfg', nx.DiGraph()),\n",
    "            submission2['ast_features'].get('control_flow', {}).get('cfg', nx.DiGraph())\n",
    "        )\n",
    "        \n",
    "        # Complexity similarity\n",
    "        complexity_sim = ScalableSimilarityAnalyzer.compute_complexity_similarity(\n",
    "            submission1['ast_features'].get('complexity', {'time_complexity': 'O(1)', 'loop_count': 0, 'max_nesting': 0}),\n",
    "            submission2['ast_features'].get('complexity', {'time_complexity': 'O(1)', 'loop_count': 0, 'max_nesting': 0})\n",
    "        )\n",
    "        \n",
    "        # Weighted combination with enhanced weights\n",
    "        weights = {\n",
    "            'tree': 0.25,\n",
    "            'hash': 0.25,\n",
    "            'structure': 0.1,\n",
    "            'cfg': 0.25,\n",
    "            'complexity': 0.15\n",
    "        }\n",
    "        \n",
    "        overall_sim = (\n",
    "            weights['tree'] * tree_sim +\n",
    "            weights['hash'] * hash_sim +\n",
    "            weights['structure'] * struct_sim +\n",
    "            weights['cfg'] * cfg_sim +\n",
    "            weights['complexity'] * complexity_sim\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'tree_edit_similarity': tree_sim,\n",
    "            'subtree_hash_similarity': hash_sim,\n",
    "            'structure_similarity': struct_sim,\n",
    "            'cfg_similarity': cfg_sim,\n",
    "            'complexity_similarity': complexity_sim,\n",
    "            'overall_similarity': overall_sim\n",
    "        }\n",
    "\n",
    "class EnhancedPlagiarismDetector:\n",
    "    def __init__(self, output_dir: str = 'plagiarism_results', \n",
    "                 similarity_threshold: float = 0.8,\n",
    "                 max_workers: int = None):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.max_workers = max_workers or os.cpu_count()\n",
    "        \n",
    "        self.submissions = {}\n",
    "        self.comparison_results = []\n",
    "        self.lsh_analyzer = ScalableSimilarityAnalyzer(threshold=similarity_threshold)\n",
    "        \n",
    "        # Set up logging for this instance\n",
    "        self.logger = logging.getLogger(f\"PlagiarismDetector_{id(self)}\")\n",
    "    \n",
    "    def _analyze_single_file(self, file_path: str) -> Optional[Dict]:\n",
    "        \"\"\"Analyze a single file with comprehensive error handling.\"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Analyzing {file_path}\")\n",
    "            \n",
    "            # Parse and analyze AST\n",
    "            ast_analyzer = ASTAnalyzer(file_path)\n",
    "            ast_features = ast_analyzer.analyze()\n",
    "            \n",
    "            # Process tokens\n",
    "            tokens = [(t.kind, t.spelling) for t in ast_analyzer.tu.cursor.get_tokens()]\n",
    "            filtered_tokens = TokenProcessor.filter_tokens(tokens)\n",
    "            normalized_tokens = TokenProcessor.normalize_identifiers(filtered_tokens)\n",
    "            \n",
    "            # Extract C++ specific patterns\n",
    "            cpp_patterns = TokenProcessor.extract_cpp_patterns(normalized_tokens)\n",
    "            \n",
    "            results = {\n",
    "                'file_path': file_path,\n",
    "                'ast_features': ast_features,\n",
    "                'token_features': {\n",
    "                    'original': tokens,\n",
    "                    'filtered': filtered_tokens,\n",
    "                    'normalized': normalized_tokens,\n",
    "                    'cpp_patterns': cpp_patterns\n",
    "                },\n",
    "                'analysis_timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            # Add to LSH index\n",
    "            token_strings = [t[1] for t in normalized_tokens]\n",
    "            self.lsh_analyzer.add_submission(file_path, token_strings)\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error analyzing {file_path}: {str(e)}\", exc_info=True)\n",
    "            return None\n",
    "    \n",
    "    def analyze_files(self, file_paths: List[str]):\n",
    "        \"\"\"Analyze multiple files in parallel.\"\"\"\n",
    "        self.logger.info(f\"Starting analysis of {len(file_paths)} files\")\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            future_to_file = {executor.submit(self._analyze_single_file, fp): fp \n",
    "                             for fp in file_paths}\n",
    "            \n",
    "            for future in concurrent.futures.as_completed(future_to_file):\n",
    "                file_path = future_to_file[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    if result:\n",
    "                        self.submissions[file_path] = result\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Analysis failed for {file_path}: {str(e)}\")\n",
    "    \n",
    "    def find_similarities(self):\n",
    "        \"\"\"Find similar submissions using LSH and detailed comparison.\"\"\"\n",
    "        self.logger.info(\"Starting similarity analysis\")\n",
    "        self.comparison_results = []\n",
    "        \n",
    "        for file_path in self.submissions:\n",
    "            # Find candidate matches using LSH\n",
    "            candidates = self.lsh_analyzer.find_similar(file_path)\n",
    "            \n",
    "            for candidate in candidates:\n",
    "                if candidate <= file_path:  # Avoid duplicate comparisons\n",
    "                    continue\n",
    "                    \n",
    "                # Detailed similarity analysis\n",
    "                similarity = self.lsh_analyzer.compute_weighted_similarity(\n",
    "                    self.submissions[file_path],\n",
    "                    self.submissions[candidate]\n",
    "                )\n",
    "                \n",
    "                if similarity['overall_similarity'] >= self.similarity_threshold:\n",
    "                    self.comparison_results.append({\n",
    "                        'file1': file_path,\n",
    "                        'file2': candidate,\n",
    "                        'similarity_metrics': similarity,\n",
    "                        'timestamp': datetime.now().isoformat()\n",
    "                    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294cc09b-c7ac-45cf-a730-0e790636ca9d",
   "metadata": {},
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize detector with adjusted threshold\n",
    "    detector = EnhancedPlagiarismDetector(\n",
    "        similarity_threshold=0.5,  # Lower threshold to catch more matches\n",
    "        max_workers=os.cpu_count()\n",
    "    )\n",
    "    \n",
    "    # Path to submissions\n",
    "    submissions_dir = r\"C:\\Users\\hp\\Desktop\\Pixel\\Plagiarism_Detection_System\\src\\B2017\"\n",
    "    \n",
    "    # Get all C/C++ files\n",
    "    cpp_files = []\n",
    "    for root, _, files in os.walk(submissions_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(('.cpp', '.c')):\n",
    "                cpp_files.append(os.path.join(root, file))\n",
    "    \n",
    "    print(f\"Found {len(cpp_files)} C/C++ files\")\n",
    "    \n",
    "    # Analyze files\n",
    "    detector.analyze_files(cpp_files)\n",
    "    \n",
    "    print(f\"Successfully analyzed {len(detector.submissions)} files\")\n",
    "    \n",
    "    # Override the similarity calculation method with a more accurate version\n",
    "    def improved_similarity(submission1, submission2):\n",
    "        \"\"\"Compute improved similarity with better weights for identical files.\"\"\"\n",
    "        try:\n",
    "            # Direct token comparison (most reliable for exact matches)\n",
    "            tokens1 = [t[1] for t in submission1['token_features']['normalized']]\n",
    "            tokens2 = [t[1] for t in submission2['token_features']['normalized']]\n",
    "            \n",
    "            # Calculate token-based similarity\n",
    "            common_tokens = set(tokens1) & set(tokens2)\n",
    "            all_tokens = set(tokens1) | set(tokens2)\n",
    "            token_sim = len(common_tokens) / len(all_tokens) if all_tokens else 0\n",
    "            \n",
    "            # Calculate sequence similarity (for exact matches)\n",
    "            min_len = min(len(tokens1), len(tokens2))\n",
    "            max_len = max(len(tokens1), len(tokens2))\n",
    "            \n",
    "            # Check for sequence matches\n",
    "            matches = 0\n",
    "            for i in range(min_len):\n",
    "                if tokens1[i] == tokens2[i]:\n",
    "                    matches += 1\n",
    "            \n",
    "            sequence_sim = matches / max_len if max_len > 0 else 0\n",
    "            \n",
    "            # Tree edit distance similarity\n",
    "            tree_sim = ScalableSimilarityAnalyzer.compute_tree_distance(\n",
    "                submission1['ast_features']['zss_tree'],\n",
    "                submission2['ast_features']['zss_tree']\n",
    "            )\n",
    "            \n",
    "            # Subtree hash similarity\n",
    "            hashes1 = set(submission1['ast_features']['subtree_hashes'].values())\n",
    "            hashes2 = set(submission2['ast_features']['subtree_hashes'].values())\n",
    "            hash_sim = len(hashes1.intersection(hashes2)) / len(hashes1.union(hashes2)) if hashes1 or hashes2 else 0\n",
    "            \n",
    "            # Structure similarity\n",
    "            struct1 = submission1['ast_features']['graph_structure']\n",
    "            struct2 = submission2['ast_features']['graph_structure']\n",
    "            struct_sim = 1 - abs(struct1['avg_branching'] - struct2['avg_branching']) / \\\n",
    "                         max(struct1['avg_branching'], struct2['avg_branching']) if max(struct1['avg_branching'], struct2['avg_branching']) > 0 else 1.0\n",
    "            \n",
    "            # Heavily weighted combination favoring token and sequence similarity\n",
    "            overall_sim = 0.4 * token_sim + 0.3 * sequence_sim + 0.15 * tree_sim + 0.1 * hash_sim + 0.05 * struct_sim\n",
    "            \n",
    "            # Confidence score based on multiple metrics agreement\n",
    "            metrics = [token_sim, sequence_sim, tree_sim, hash_sim, struct_sim]\n",
    "            avg = sum(metrics) / len(metrics)\n",
    "            variance = sum((m - avg) ** 2 for m in metrics) / len(metrics)\n",
    "            confidence = 1 - (variance * 2)  # Lower variance means higher confidence\n",
    "            \n",
    "            return {\n",
    "                'token_similarity': token_sim,\n",
    "                'sequence_similarity': sequence_sim,\n",
    "                'tree_edit_similarity': tree_sim,\n",
    "                'subtree_hash_similarity': hash_sim,\n",
    "                'structure_similarity': struct_sim,\n",
    "                'overall_similarity': overall_sim,\n",
    "                'confidence': max(0, min(1, confidence))  # Clamp between 0 and 1\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error in similarity calculation: {e}\")\n",
    "            # Fallback to basic similarity\n",
    "            return {\n",
    "                'overall_similarity': 0.0,\n",
    "                'tree_edit_similarity': 0.0,\n",
    "                'subtree_hash_similarity': 0.0,\n",
    "                'structure_similarity': 0.0,\n",
    "                'confidence': 0.0\n",
    "            }\n",
    "    \n",
    "    # Replace the similarity calculation method\n",
    "    detector.lsh_analyzer.compute_weighted_similarity = staticmethod(improved_similarity)\n",
    "    \n",
    "    # Find similarities\n",
    "    detector.find_similarities()\n",
    "    \n",
    "    print(f\"Found {len(detector.comparison_results)} potential similarity matches\")\n",
    "    \n",
    "    # Save results\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Save CSV summary\n",
    "    csv_file = detector.output_dir / f'summary_{timestamp}.csv'\n",
    "    with open(csv_file, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            'File 1', 'File 2', 'Overall Similarity',\n",
    "            'Token Similarity', 'Sequence Similarity',\n",
    "            'Tree Edit Similarity', 'Subtree Hash Similarity',\n",
    "            'Structure Similarity', 'Confidence'\n",
    "        ])\n",
    "        \n",
    "        for result in detector.comparison_results:\n",
    "            metrics = result['similarity_metrics']\n",
    "            writer.writerow([\n",
    "                os.path.basename(result['file1']),\n",
    "                os.path.basename(result['file2']),\n",
    "                metrics['overall_similarity'],\n",
    "                metrics.get('token_similarity', 'N/A'),\n",
    "                metrics.get('sequence_similarity', 'N/A'),\n",
    "                metrics['tree_edit_similarity'],\n",
    "                metrics['subtree_hash_similarity'],\n",
    "                metrics['structure_similarity'],\n",
    "                metrics.get('confidence', 'N/A')\n",
    "            ])\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nAnalysis complete! Results saved to: {csv_file}\")\n",
    "    \n",
    "    if detector.comparison_results:\n",
    "        print(f\"\\nFound {len(detector.comparison_results)} potential cases of plagiarism:\")\n",
    "        \n",
    "        # Sort by similarity\n",
    "        sorted_results = sorted(\n",
    "            detector.comparison_results,\n",
    "            key=lambda x: x['similarity_metrics']['overall_similarity'],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        for i, result in enumerate(sorted_results[:10]):  # Show top 10\n",
    "            metrics = result['similarity_metrics']\n",
    "            print(f\"\\n{i+1}. Files:\")\n",
    "            print(f\"   - {os.path.basename(result['file1'])}\")\n",
    "            print(f\"   - {os.path.basename(result['file2'])}\")\n",
    "            print(f\"   Overall Similarity: {metrics['overall_similarity']:.2%}\")\n",
    "            if 'token_similarity' in metrics:\n",
    "                print(f\"   Token Similarity: {metrics['token_similarity']:.2%}\")\n",
    "            if 'sequence_similarity' in metrics:\n",
    "                print(f\"   Sequence Similarity: {metrics['sequence_similarity']:.2%}\")\n",
    "            print(f\"   Tree Edit Similarity: {metrics['tree_edit_similarity']:.2%}\")\n",
    "            print(f\"   Subtree Hash Similarity: {metrics['subtree_hash_similarity']:.2%}\")\n",
    "            print(f\"   Structure Similarity: {metrics['structure_similarity']:.2%}\")\n",
    "            if 'confidence' in metrics:\n",
    "                print(f\"   Confidence: {metrics['confidence']:.2%}\")\n",
    "            \n",
    "            # Check for exact duplicates\n",
    "            if metrics['overall_similarity'] > 0.95:\n",
    "                print(\"   ⚠️ EXACT DUPLICATE DETECTED ⚠️\")\n",
    "        \n",
    "        if len(sorted_results) > 10:\n",
    "            print(f\"\\n... and {len(sorted_results) - 10} more cases (see CSV file for details)\")\n",
    "    else:\n",
    "        print(\"\\nNo suspicious similarities found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e6e3347-64e3-413e-b031-8ec628fccaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tkinter as tk\n",
    "from tkinter import ttk, filedialog, scrolledtext, messagebox\n",
    "from datetime import datetime\n",
    "import threading\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "class PlagiarismDetectorGUI:\n",
    "    def __init__(self, root, notebook_module):\n",
    "        self.root = root\n",
    "        self.root.title(\"Plagiarism Detection System\")\n",
    "        self.root.geometry(\"800x600\")\n",
    "        self.root.minsize(800, 600)\n",
    "        \n",
    "        # Import notebook module with all the classes\n",
    "        self.nb_module = notebook_module\n",
    "        \n",
    "        # Set up the main frame\n",
    "        self.main_frame = ttk.Frame(root, padding=\"10\")\n",
    "        self.main_frame.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        # Create tabs\n",
    "        self.tab_control = ttk.Notebook(self.main_frame)\n",
    "        \n",
    "        # File comparison tab\n",
    "        self.file_tab = ttk.Frame(self.tab_control)\n",
    "        self.tab_control.add(self.file_tab, text=\"File Comparison\")\n",
    "        \n",
    "        # Bulk analysis tab\n",
    "        self.bulk_tab = ttk.Frame(self.tab_control)\n",
    "        self.tab_control.add(self.bulk_tab, text=\"Bulk Analysis\")\n",
    "        \n",
    "        self.tab_control.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        # Set up the file comparison tab\n",
    "        self.setup_file_comparison_tab()\n",
    "        \n",
    "        # Set up the bulk analysis tab\n",
    "        self.setup_bulk_analysis_tab()\n",
    "        \n",
    "        # Status bar\n",
    "        self.status_var = tk.StringVar()\n",
    "        self.status_var.set(\"Ready\")\n",
    "        self.status_bar = ttk.Label(root, textvariable=self.status_var, relief=tk.SUNKEN, anchor=tk.W)\n",
    "        self.status_bar.pack(side=tk.BOTTOM, fill=tk.X)\n",
    "        \n",
    "        # Progress bar\n",
    "        self.progress = ttk.Progressbar(root, orient=tk.HORIZONTAL, length=100, mode='indeterminate')\n",
    "        self.progress.pack(side=tk.BOTTOM, fill=tk.X, padx=10, pady=5)\n",
    "        \n",
    "        # Initialize the detector\n",
    "        self.detector = None\n",
    "        self.initialize_detector()\n",
    "    \n",
    "    def initialize_detector(self):\n",
    "        try:\n",
    "            # Create an optimized detector using classes from the notebook\n",
    "            self.detector = self.nb_module.EnhancedPlagiarismDetector(\n",
    "                similarity_threshold=0.7,\n",
    "                max_workers=os.cpu_count()\n",
    "            )\n",
    "            \n",
    "            # Define optimized similarity calculation\n",
    "            def optimized_similarity(submission1, submission2):\n",
    "                try:\n",
    "                    # Direct content comparison for exact matches\n",
    "                    file1 = submission1['file_path']\n",
    "                    file2 = submission2['file_path']\n",
    "                    \n",
    "                    try:\n",
    "                        with open(file1, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                            content1 = ''.join(f.read().split())\n",
    "                        with open(file2, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                            content2 = ''.join(f.read().split())\n",
    "                            \n",
    "                        if content1 == content2:\n",
    "                            return {\n",
    "                                'direct_match': 1.0,\n",
    "                                'token_similarity': 1.0,\n",
    "                                'tree_edit_similarity': 1.0,\n",
    "                                'subtree_hash_similarity': 1.0,\n",
    "                                'structure_similarity': 1.0,\n",
    "                                'overall_similarity': 1.0\n",
    "                            }\n",
    "                    except:\n",
    "                        pass  # Continue with other similarity metrics\n",
    "                    \n",
    "                    # Token comparison\n",
    "                    tokens1 = [t[1] for t in submission1['token_features']['normalized']]\n",
    "                    tokens2 = [t[1] for t in submission2['token_features']['normalized']]\n",
    "                    \n",
    "                    # Calculate token similarity\n",
    "                    common_tokens = set(tokens1) & set(tokens2)\n",
    "                    all_tokens = set(tokens1) | set(tokens2)\n",
    "                    token_sim = len(common_tokens) / len(all_tokens) if all_tokens else 0\n",
    "                    \n",
    "                    # Calculate sequence similarity\n",
    "                    min_len = min(len(tokens1), len(tokens2))\n",
    "                    max_len = max(len(tokens1), len(tokens2))\n",
    "                    matches = sum(1 for i in range(min_len) if tokens1[i] == tokens2[i])\n",
    "                    sequence_sim = matches / max_len if max_len > 0 else 0\n",
    "                    \n",
    "                    # Tree edit distance\n",
    "                    tree_sim = self.nb_module.ScalableSimilarityAnalyzer.compute_tree_distance(\n",
    "                        submission1['ast_features']['zss_tree'],\n",
    "                        submission2['ast_features']['zss_tree']\n",
    "                    )\n",
    "                    \n",
    "                    # Subtree hash similarity\n",
    "                    hashes1 = set(submission1['ast_features']['subtree_hashes'].values())\n",
    "                    hashes2 = set(submission2['ast_features']['subtree_hashes'].values())\n",
    "                    hash_sim = len(hashes1.intersection(hashes2)) / len(hashes1.union(hashes2)) if hashes1 or hashes2 else 0\n",
    "                    \n",
    "                    # Structure similarity\n",
    "                    struct1 = submission1['ast_features']['graph_structure']\n",
    "                    struct2 = submission2['ast_features']['graph_structure']\n",
    "                    struct_sim = 1 - abs(struct1['avg_branching'] - struct2['avg_branching']) / \\\n",
    "                                max(struct1['avg_branching'], struct2['avg_branching']) if max(struct1['avg_branching'], struct2['avg_branching']) > 0 else 1.0\n",
    "                    \n",
    "                    # Weighted combination\n",
    "                    overall_sim = 0.3 * token_sim + 0.1 * sequence_sim + 0.3 * tree_sim + 0.2 * hash_sim + 0.1 * struct_sim\n",
    "                    \n",
    "                    # Calculate confidence based on agreement of metrics\n",
    "                    metrics = [token_sim, tree_sim, hash_sim, struct_sim]\n",
    "                    avg = sum(metrics) / len(metrics)\n",
    "                    variance = sum((m - avg) ** 2 for m in metrics) / len(metrics)\n",
    "                    confidence = 1 - (variance * 2)  # Lower variance means higher confidence\n",
    "                    confidence = max(0, min(1, confidence))  # Clamp to [0,1]\n",
    "                    \n",
    "                    return {\n",
    "                        'token_similarity': token_sim,\n",
    "                        'sequence_similarity': sequence_sim,\n",
    "                        'tree_edit_similarity': tree_sim,\n",
    "                        'subtree_hash_similarity': hash_sim,\n",
    "                        'structure_similarity': struct_sim,\n",
    "                        'overall_similarity': overall_sim,\n",
    "                        'confidence': confidence\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in similarity calculation: {e}\")\n",
    "                    return {\n",
    "                        'overall_similarity': 0.0,\n",
    "                        'tree_edit_similarity': 0.0,\n",
    "                        'subtree_hash_similarity': 0.0,\n",
    "                        'structure_similarity': 0.0,\n",
    "                        'confidence': 0.0\n",
    "                    }\n",
    "            \n",
    "            # Override the similarity calculation\n",
    "            self.detector.lsh_analyzer.compute_weighted_similarity = staticmethod(optimized_similarity)\n",
    "            self.status_var.set(\"Detector initialized successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.status_var.set(f\"Initialization error: {str(e)}\")\n",
    "            print(f\"Initialization error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "007ec698-8997-43f6-be1d-7939fbc000cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 12:46:50,587 - INFO - Starting analysis of 5 files\n",
      "2025-05-25 12:46:50,594 - INFO - Analyzing E:/DAA_Project/src/Samples\\sample1.cpp\n",
      "2025-05-25 12:46:50,598 - INFO - Analyzing E:/DAA_Project/src/Samples\\sample2.cpp\n",
      "2025-05-25 12:46:50,614 - INFO - Analyzing E:/DAA_Project/src/Samples\\sample3.cpp\n",
      "2025-05-25 12:46:50,617 - INFO - Analyzing E:/DAA_Project/src/Samples\\sample4.cpp\n",
      "2025-05-25 12:46:50,618 - INFO - Analyzing E:/DAA_Project/src/Samples\\sample5.cpp\n",
      "2025-05-25 12:46:51,405 - INFO - Starting similarity analysis\n"
     ]
    }
   ],
   "source": [
    "# GUI for Plagiarism Detection System\n",
    "def create_gui():\n",
    "    import tkinter as tk\n",
    "    from tkinter import ttk, filedialog, scrolledtext, messagebox\n",
    "    import threading\n",
    "    import csv\n",
    "    from pathlib import Path\n",
    "    import os\n",
    "    \n",
    "    # Create the main window\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Plagiarism Detection System\")\n",
    "    root.geometry(\"900x700\")\n",
    "    root.minsize(800, 600)\n",
    "    \n",
    "    # Create a detector instance\n",
    "    detector = EnhancedPlagiarismDetector(\n",
    "        similarity_threshold=0.5,\n",
    "        max_workers=os.cpu_count()\n",
    "    )\n",
    "    \n",
    "    # Add improved similarity method\n",
    "    def improved_similarity(submission1, submission2):\n",
    "        \"\"\"Compute improved similarity with better weights for identical files.\"\"\"\n",
    "        try:\n",
    "            # Direct token comparison (most reliable for exact matches)\n",
    "            tokens1 = [t[1] for t in submission1['token_features']['normalized']]\n",
    "            tokens2 = [t[1] for t in submission2['token_features']['normalized']]\n",
    "            \n",
    "            # Calculate token-based similarity\n",
    "            common_tokens = set(tokens1) & set(tokens2)\n",
    "            all_tokens = set(tokens1) | set(tokens2)\n",
    "            token_sim = len(common_tokens) / len(all_tokens) if all_tokens else 0\n",
    "            \n",
    "            # Calculate sequence similarity (for exact matches)\n",
    "            min_len = min(len(tokens1), len(tokens2))\n",
    "            max_len = max(len(tokens1), len(tokens2))\n",
    "            \n",
    "            # Check for sequence matches\n",
    "            matches = 0\n",
    "            for i in range(min_len):\n",
    "                if tokens1[i] == tokens2[i]:\n",
    "                    matches += 1\n",
    "            \n",
    "            sequence_sim = matches / max_len if max_len > 0 else 0\n",
    "            \n",
    "            # Tree edit distance similarity\n",
    "            tree_sim = ScalableSimilarityAnalyzer.compute_tree_distance(\n",
    "                submission1['ast_features']['zss_tree'],\n",
    "                submission2['ast_features']['zss_tree']\n",
    "            )\n",
    "            \n",
    "            # Subtree hash similarity\n",
    "            hashes1 = set(submission1['ast_features']['subtree_hashes'].values())\n",
    "            hashes2 = set(submission2['ast_features']['subtree_hashes'].values())\n",
    "            hash_sim = len(hashes1.intersection(hashes2)) / len(hashes1.union(hashes2)) if hashes1 or hashes2 else 0\n",
    "            \n",
    "            # Structure similarity\n",
    "            struct1 = submission1['ast_features']['graph_structure']\n",
    "            struct2 = submission2['ast_features']['graph_structure']\n",
    "            struct_sim = 1 - abs(struct1['avg_branching'] - struct2['avg_branching']) / \\\n",
    "                         max(struct1['avg_branching'], struct2['avg_branching']) if max(struct1['avg_branching'], struct2['avg_branching']) > 0 else 1.0\n",
    "            \n",
    "            # Heavily weighted combination favoring token and sequence similarity\n",
    "            overall_sim = 0.4 * token_sim + 0.3 * sequence_sim + 0.15 * tree_sim + 0.1 * hash_sim + 0.05 * struct_sim\n",
    "            \n",
    "            # Confidence score based on multiple metrics agreement\n",
    "            metrics = [token_sim, sequence_sim, tree_sim, hash_sim, struct_sim]\n",
    "            avg = sum(metrics) / len(metrics)\n",
    "            variance = sum((m - avg) ** 2 for m in metrics) / len(metrics)\n",
    "            confidence = 1 - (variance * 2)  # Lower variance means higher confidence\n",
    "            \n",
    "            return {\n",
    "                'token_similarity': token_sim,\n",
    "                'sequence_similarity': sequence_sim,\n",
    "                'tree_edit_similarity': tree_sim,\n",
    "                'subtree_hash_similarity': hash_sim,\n",
    "                'structure_similarity': struct_sim,\n",
    "                'overall_similarity': overall_sim,\n",
    "                'confidence': max(0, min(1, confidence))  # Clamp between 0 and 1\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error in similarity calculation: {e}\")\n",
    "            # Fallback to basic similarity\n",
    "            return {\n",
    "                'overall_similarity': 0.0,\n",
    "                'tree_edit_similarity': 0.0,\n",
    "                'subtree_hash_similarity': 0.0,\n",
    "                'structure_similarity': 0.0,\n",
    "                'confidence': 0.0\n",
    "            }\n",
    "    \n",
    "    # Replace the similarity calculation method\n",
    "    detector.lsh_analyzer.compute_weighted_similarity = staticmethod(improved_similarity)\n",
    "    \n",
    "    # Main frame with notebook (tabs)\n",
    "    main_frame = ttk.Frame(root, padding=10)\n",
    "    main_frame.pack(fill=tk.BOTH, expand=True)\n",
    "    \n",
    "    notebook = ttk.Notebook(main_frame)\n",
    "    notebook.pack(fill=tk.BOTH, expand=True)\n",
    "    \n",
    "    # File comparison tab\n",
    "    file_tab = ttk.Frame(notebook)\n",
    "    notebook.add(file_tab, text=\"File Comparison\")\n",
    "    \n",
    "    # Directory analysis tab\n",
    "    dir_tab = ttk.Frame(notebook)\n",
    "    notebook.add(dir_tab, text=\"Directory Analysis\")\n",
    "    \n",
    "    # Status bar and progress bar\n",
    "    status_var = tk.StringVar(value=\"Ready\")\n",
    "    progress = ttk.Progressbar(root, orient=\"horizontal\", mode=\"indeterminate\")\n",
    "    progress.pack(side=tk.BOTTOM, fill=tk.X, padx=10, pady=5)\n",
    "    status_bar = ttk.Label(root, textvariable=status_var, relief=tk.SUNKEN, anchor=tk.W)\n",
    "    status_bar.pack(side=tk.BOTTOM, fill=tk.X)\n",
    "    \n",
    "    # ===== File Comparison Tab =====\n",
    "    file_frame = ttk.LabelFrame(file_tab, text=\"Select Files\", padding=10)\n",
    "    file_frame.pack(fill=tk.X, padx=10, pady=10)\n",
    "    \n",
    "    # File 1\n",
    "    file1_var = tk.StringVar()\n",
    "    ttk.Label(file_frame, text=\"File 1:\").grid(row=0, column=0, sticky=tk.W, pady=5)\n",
    "    ttk.Entry(file_frame, textvariable=file1_var, width=60).grid(row=0, column=1, padx=5, pady=5)\n",
    "    \n",
    "    def browse_file1():\n",
    "        filename = filedialog.askopenfilename(filetypes=[(\"C/C++ Files\", \"*.c;*.cpp\"), (\"All Files\", \"*.*\")])\n",
    "        if filename:\n",
    "            file1_var.set(filename)\n",
    "    \n",
    "    ttk.Button(file_frame, text=\"Browse\", command=browse_file1).grid(row=0, column=2, padx=5, pady=5)\n",
    "    \n",
    "    # File 2\n",
    "    file2_var = tk.StringVar()\n",
    "    ttk.Label(file_frame, text=\"File 2:\").grid(row=1, column=0, sticky=tk.W, pady=5)\n",
    "    ttk.Entry(file_frame, textvariable=file2_var, width=60).grid(row=1, column=1, padx=5, pady=5)\n",
    "    \n",
    "    def browse_file2():\n",
    "        filename = filedialog.askopenfilename(filetypes=[(\"C/C++ Files\", \"*.c;*.cpp\"), (\"All Files\", \"*.*\")])\n",
    "        if filename:\n",
    "            file2_var.set(filename)\n",
    "    \n",
    "    ttk.Button(file_frame, text=\"Browse\", command=browse_file2).grid(row=1, column=2, padx=5, pady=5)\n",
    "    \n",
    "    # Results area\n",
    "    results_frame = ttk.LabelFrame(file_tab, text=\"Comparison Results\", padding=10)\n",
    "    results_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n",
    "    \n",
    "    # Similarity meter\n",
    "    similarity_frame = ttk.Frame(results_frame)\n",
    "    similarity_frame.pack(fill=tk.X, padx=5, pady=10)\n",
    "    \n",
    "    similarity_label_var = tk.StringVar(value=\"Similarity: 0.00%\")\n",
    "    similarity_label = ttk.Label(similarity_frame, textvariable=similarity_label_var, font=(\"Arial\", 12, \"bold\"))\n",
    "    similarity_label.pack(side=tk.TOP, pady=5)\n",
    "    \n",
    "    similarity_meter = ttk.Progressbar(similarity_frame, orient=\"horizontal\", length=300, mode=\"determinate\")\n",
    "    similarity_meter.pack(side=tk.TOP, pady=5)\n",
    "    \n",
    "    # Detailed results\n",
    "    results_text = scrolledtext.ScrolledText(results_frame, wrap=tk.WORD, height=20)\n",
    "    results_text.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "    results_text.config(state=tk.DISABLED)\n",
    "    \n",
    "    def compare_files():\n",
    "        file1 = file1_var.get()\n",
    "        file2 = file2_var.get()\n",
    "        \n",
    "        if not file1 or not file2:\n",
    "            messagebox.showerror(\"Error\", \"Please select two files to compare\")\n",
    "            return\n",
    "        \n",
    "        if not os.path.exists(file1) or not os.path.exists(file2):\n",
    "            messagebox.showerror(\"Error\", \"One or both selected files do not exist\")\n",
    "            return\n",
    "        \n",
    "        # Clear previous results\n",
    "        results_text.config(state=tk.NORMAL)\n",
    "        results_text.delete(1.0, tk.END)\n",
    "        results_text.config(state=tk.DISABLED)\n",
    "        \n",
    "        # Update UI\n",
    "        status_var.set(\"Comparing files...\")\n",
    "        progress.start()\n",
    "        \n",
    "        def run_comparison():\n",
    "            try:\n",
    "                # Reset detector for clean comparison\n",
    "                detector.submissions = {}\n",
    "                \n",
    "                # Analyze files\n",
    "                detector.analyze_files([file1, file2])\n",
    "                \n",
    "                # Check if files were analyzed successfully\n",
    "                if file1 not in detector.submissions or file2 not in detector.submissions:\n",
    "                    root.after(0, lambda: update_status(\"Error: Failed to analyze one or both files\"))\n",
    "                    return\n",
    "                \n",
    "                # Calculate similarity\n",
    "                similarity = detector.lsh_analyzer.compute_weighted_similarity(\n",
    "                    detector.submissions[file1],\n",
    "                    detector.submissions[file2]\n",
    "                )\n",
    "                \n",
    "                # Update UI with results\n",
    "                root.after(0, lambda: display_results(file1, file2, similarity))\n",
    "                \n",
    "            except Exception as e:\n",
    "                root.after(0, lambda: update_status(f\"Error: {str(e)}\"))\n",
    "        \n",
    "        # Run comparison in a separate thread\n",
    "        threading.Thread(target=run_comparison, daemon=True).start()\n",
    "    \n",
    "    def display_results(file1, file2, similarity):\n",
    "        # Stop progress and update status\n",
    "        progress.stop()\n",
    "        status_var.set(\"Comparison complete\")\n",
    "        \n",
    "        # Update similarity meter\n",
    "        overall_sim = similarity['overall_similarity'] * 100\n",
    "        similarity_label_var.set(f\"Similarity: {overall_sim:.2f}%\")\n",
    "        similarity_meter['value'] = overall_sim\n",
    "        \n",
    "        # Update detailed results\n",
    "        results_text.config(state=tk.NORMAL)\n",
    "        \n",
    "        # File info\n",
    "        results_text.insert(tk.END, f\"File 1: {os.path.basename(file1)}\\n\")\n",
    "        results_text.insert(tk.END, f\"File 2: {os.path.basename(file2)}\\n\\n\")\n",
    "        \n",
    "        # Overall similarity with color coding\n",
    "        results_text.insert(tk.END, \"Overall Similarity: \")\n",
    "        if overall_sim >= 90:\n",
    "            results_text.insert(tk.END, f\"{overall_sim:.2f}%\\n\", \"high\")\n",
    "            results_text.insert(tk.END, \"⚠️ HIGH PROBABILITY OF PLAGIARISM ⚠️\\n\\n\", \"warning\")\n",
    "        elif overall_sim >= 70:\n",
    "            results_text.insert(tk.END, f\"{overall_sim:.2f}%\\n\", \"medium\")\n",
    "            results_text.insert(tk.END, \"⚠️ SUSPICIOUS SIMILARITY DETECTED ⚠️\\n\\n\", \"warning\")\n",
    "        else:\n",
    "            results_text.insert(tk.END, f\"{overall_sim:.2f}%\\n\", \"low\")\n",
    "        \n",
    "        # Add confidence if available\n",
    "        if 'confidence' in similarity:\n",
    "            confidence = similarity['confidence'] * 100\n",
    "            results_text.insert(tk.END, f\"Confidence: {confidence:.2f}%\\n\\n\")\n",
    "        \n",
    "        # Detailed metrics\n",
    "        results_text.insert(tk.END, \"Detailed Metrics:\\n\")\n",
    "        \n",
    "        if 'token_similarity' in similarity:\n",
    "            token_sim = similarity['token_similarity'] * 100\n",
    "            results_text.insert(tk.END, f\"Token Similarity: {token_sim:.2f}%\\n\")\n",
    "            \n",
    "        if 'sequence_similarity' in similarity:\n",
    "            seq_sim = similarity['sequence_similarity'] * 100\n",
    "            results_text.insert(tk.END, f\"Sequence Similarity: {seq_sim:.2f}%\\n\")\n",
    "            \n",
    "        if 'tree_edit_similarity' in similarity:\n",
    "            tree_sim = similarity['tree_edit_similarity'] * 100\n",
    "            results_text.insert(tk.END, f\"Tree Edit Similarity: {tree_sim:.2f}%\\n\")\n",
    "            \n",
    "        if 'subtree_hash_similarity' in similarity:\n",
    "            hash_sim = similarity['subtree_hash_similarity'] * 100\n",
    "            results_text.insert(tk.END, f\"Subtree Hash Similarity: {hash_sim:.2f}%\\n\")\n",
    "            \n",
    "        if 'structure_similarity' in similarity:\n",
    "            struct_sim = similarity['structure_similarity'] * 100\n",
    "            results_text.insert(tk.END, f\"Structure Similarity: {struct_sim:.2f}%\\n\")\n",
    "        \n",
    "        # Set up text tags for coloring\n",
    "        results_text.tag_configure(\"high\", foreground=\"red\", font=(\"Arial\", 11, \"bold\"))\n",
    "        results_text.tag_configure(\"medium\", foreground=\"orange\", font=(\"Arial\", 11, \"bold\"))\n",
    "        results_text.tag_configure(\"low\", foreground=\"green\", font=(\"Arial\", 11, \"bold\"))\n",
    "        results_text.tag_configure(\"warning\", foreground=\"red\", font=(\"Arial\", 12, \"bold\"))\n",
    "        \n",
    "        results_text.config(state=tk.DISABLED)\n",
    "    \n",
    "    # Compare button\n",
    "    ttk.Button(file_frame, text=\"Compare Files\", command=compare_files).grid(row=2, column=1, pady=10)\n",
    "    \n",
    "    # ===== Directory Analysis Tab =====\n",
    "    dir_frame = ttk.LabelFrame(dir_tab, text=\"Directory Analysis\", padding=10)\n",
    "    dir_frame.pack(fill=tk.X, padx=10, pady=10)\n",
    "    \n",
    "    # Directory path\n",
    "    dir_path_var = tk.StringVar()\n",
    "    ttk.Label(dir_frame, text=\"Directory:\").grid(row=0, column=0, sticky=tk.W, pady=5)\n",
    "    ttk.Entry(dir_frame, textvariable=dir_path_var, width=60).grid(row=0, column=1, padx=5, pady=5)\n",
    "    \n",
    "    def browse_dir():\n",
    "        dirname = filedialog.askdirectory()\n",
    "        if dirname:\n",
    "            dir_path_var.set(dirname)\n",
    "    \n",
    "    ttk.Button(dir_frame, text=\"Browse\", command=browse_dir).grid(row=0, column=2, padx=5, pady=5)\n",
    "    \n",
    "    # Options frame\n",
    "    options_frame = ttk.LabelFrame(dir_frame, text=\"Options\")\n",
    "    options_frame.grid(row=1, column=0, columnspan=3, sticky=tk.EW, padx=5, pady=5)\n",
    "    \n",
    "    # Threshold\n",
    "    ttk.Label(options_frame, text=\"Similarity Threshold:\").grid(row=0, column=0, sticky=tk.W, padx=5, pady=5)\n",
    "    threshold_var = tk.DoubleVar(value=0.5)\n",
    "    threshold_spinbox = ttk.Spinbox(options_frame, from_=0.1, to=1.0, increment=0.05, textvariable=threshold_var, width=5)\n",
    "    threshold_spinbox.grid(row=0, column=1, padx=5, pady=5)\n",
    "    \n",
    "    # Max files\n",
    "    ttk.Label(options_frame, text=\"Max Files (0 for all):\").grid(row=0, column=2, sticky=tk.W, padx=15, pady=5)\n",
    "    max_files_var = tk.IntVar(value=0)\n",
    "    max_files_spinbox = ttk.Spinbox(options_frame, from_=0, to=1000, increment=10, textvariable=max_files_var, width=5)\n",
    "    max_files_spinbox.grid(row=0, column=3, padx=5, pady=5)\n",
    "    \n",
    "    # Results treeview\n",
    "    results_tree_frame = ttk.LabelFrame(dir_tab, text=\"Results\", padding=10)\n",
    "    results_tree_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n",
    "    \n",
    "    # Create treeview with scrollbars\n",
    "    tree_columns = (\"file1\", \"file2\", \"similarity\", \"confidence\", \"match_type\")\n",
    "    results_tree = ttk.Treeview(results_tree_frame, columns=tree_columns, show=\"headings\")\n",
    "    \n",
    "    # Set column headings\n",
    "    results_tree.heading(\"file1\", text=\"File 1\")\n",
    "    results_tree.heading(\"file2\", text=\"File 2\")\n",
    "    results_tree.heading(\"similarity\", text=\"Similarity\")\n",
    "    results_tree.heading(\"confidence\", text=\"Confidence\")\n",
    "    results_tree.heading(\"match_type\", text=\"Match Type\")\n",
    "    \n",
    "    # Set column widths\n",
    "    results_tree.column(\"file1\", width=180)\n",
    "    results_tree.column(\"file2\", width=180)\n",
    "    results_tree.column(\"similarity\", width=80, anchor=tk.CENTER)\n",
    "    results_tree.column(\"confidence\", width=80, anchor=tk.CENTER)\n",
    "    results_tree.column(\"match_type\", width=150, anchor=tk.CENTER)\n",
    "    \n",
    "    # Add scrollbars\n",
    "    tree_vsb = ttk.Scrollbar(results_tree_frame, orient=\"vertical\", command=results_tree.yview)\n",
    "    tree_hsb = ttk.Scrollbar(results_tree_frame, orient=\"horizontal\", command=results_tree.xview)\n",
    "    results_tree.configure(yscrollcommand=tree_vsb.set, xscrollcommand=tree_hsb.set)\n",
    "    \n",
    "    # Grid layout for treeview and scrollbars\n",
    "    results_tree.grid(row=0, column=0, sticky=tk.NSEW)\n",
    "    tree_vsb.grid(row=0, column=1, sticky=tk.NS)\n",
    "    tree_hsb.grid(row=1, column=0, sticky=tk.EW)\n",
    "    \n",
    "    # Configure grid weights\n",
    "    results_tree_frame.columnconfigure(0, weight=1)\n",
    "    results_tree_frame.rowconfigure(0, weight=1)\n",
    "    \n",
    "    # Configure tags for colors\n",
    "    results_tree.tag_configure(\"high\", background=\"#ffcccc\")\n",
    "    results_tree.tag_configure(\"medium\", background=\"#ffffcc\")\n",
    "    results_tree.tag_configure(\"low\", background=\"#ccffcc\")\n",
    "    \n",
    "    def analyze_directory():\n",
    "        directory = dir_path_var.get()\n",
    "        \n",
    "        if not directory or not os.path.isdir(directory):\n",
    "            messagebox.showerror(\"Error\", \"Please select a valid directory\")\n",
    "            return\n",
    "        \n",
    "        # Clear previous results\n",
    "        for item in results_tree.get_children():\n",
    "            results_tree.delete(item)\n",
    "        \n",
    "        # Get options\n",
    "        threshold = threshold_var.get()\n",
    "        max_files = max_files_var.get()\n",
    "        \n",
    "        # Update UI\n",
    "        status_var.set(\"Starting analysis...\")\n",
    "        progress.start()\n",
    "        \n",
    "        def run_analysis():\n",
    "            try:\n",
    "                # Reset detector\n",
    "                detector.submissions = {}\n",
    "                detector.comparison_results = []\n",
    "                detector.similarity_threshold = threshold\n",
    "                \n",
    "                # Get all C/C++ files\n",
    "                cpp_files = []\n",
    "                for root_dir, _, files in os.walk(directory):\n",
    "                    for file in files:\n",
    "                        if file.endswith(('.cpp', '.c')):\n",
    "                            cpp_files.append(os.path.join(root_dir, file))\n",
    "        \n",
    "                # Limit files if needed\n",
    "                if max_files > 0 and len(cpp_files) > max_files:\n",
    "                    cpp_files = cpp_files[:max_files]\n",
    "                \n",
    "                # Update status using root.after for thread safety\n",
    "                file_count = len(cpp_files)\n",
    "                root.after(0, lambda: status_var.set(f\"Analyzing {file_count} files...\"))\n",
    "                \n",
    "                # Analyze files\n",
    "                detector.analyze_files(cpp_files)\n",
    "                \n",
    "                # Find similarities using root.after\n",
    "                root.after(0, lambda: status_var.set(\"Finding similarities...\"))\n",
    "                detector.find_similarities()\n",
    "                \n",
    "                # Sort results by similarity\n",
    "                sorted_results = sorted(\n",
    "                    detector.comparison_results,\n",
    "                    key=lambda x: x['similarity_metrics']['overall_similarity'],\n",
    "                    reverse=True\n",
    "                )\n",
    "        \n",
    "                # Send results to main thread using root.after\n",
    "                root.after(0, lambda: display_directory_results(sorted_results))\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Report error using root.after\n",
    "                root.after(0, lambda: update_status(f\"Error: {str(e)}\"))\n",
    "        \n",
    "        # Run analysis in a separate thread\n",
    "        threading.Thread(target=run_analysis, daemon=True).start()\n",
    "    \n",
    "    def display_directory_results(sorted_results=None):\n",
    "        # Stop progress\n",
    "        progress.stop()\n",
    "        \n",
    "        # If no results were provided, use the detector's results\n",
    "        if sorted_results is None:\n",
    "            sorted_results = sorted(\n",
    "                detector.comparison_results,\n",
    "                key=lambda x: x['similarity_metrics']['overall_similarity'],\n",
    "                reverse=True\n",
    "            )\n",
    "        \n",
    "        # Add to treeview\n",
    "        for result in sorted_results:\n",
    "            metrics = result['similarity_metrics']\n",
    "            overall_sim = metrics['overall_similarity'] * 100\n",
    "            confidence = metrics.get('confidence', 0.5) * 100\n",
    "                \n",
    "            # Determine match type\n",
    "            if overall_sim >= 90:\n",
    "                match_type = \"HIGH SIMILARITY\"\n",
    "            elif overall_sim >= 70:\n",
    "                match_type = \"SUSPICIOUS\"\n",
    "            else:\n",
    "                match_type = \"LOW SIMILARITY\"\n",
    "            \n",
    "            # Insert into tree\n",
    "            item_id = results_tree.insert(\"\", tk.END, values=(\n",
    "                os.path.basename(result['file1']),\n",
    "                os.path.basename(result['file2']),\n",
    "                f\"{overall_sim:.2f}%\",\n",
    "                f\"{confidence:.2f}%\",\n",
    "                match_type\n",
    "            ))\n",
    "            \n",
    "            # Color based on similarity\n",
    "            if overall_sim >= 90:\n",
    "                results_tree.item(item_id, tags=(\"high\",))\n",
    "            elif overall_sim >= 70:\n",
    "                results_tree.item(item_id, tags=(\"medium\",))\n",
    "            else:\n",
    "                results_tree.item(item_id, tags=(\"low\",))\n",
    "        \n",
    "        # Update status\n",
    "        update_status(f\"Analysis complete. Found {len(sorted_results)} potential matches.\")\n",
    "    \n",
    "    def export_results():\n",
    "        if not hasattr(detector, 'comparison_results') or not detector.comparison_results:\n",
    "            messagebox.showinfo(\"Info\", \"No results to export\")\n",
    "            return\n",
    "        \n",
    "        # Ask for save location\n",
    "        filename = filedialog.asksaveasfilename(\n",
    "            defaultextension=\".csv\",\n",
    "            filetypes=[(\"CSV files\", \"*.csv\"), (\"All files\", \"*.*\")]\n",
    "        )\n",
    "        \n",
    "        if not filename:\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            # Sort results by similarity\n",
    "            sorted_results = sorted(\n",
    "                detector.comparison_results,\n",
    "                key=lambda x: x['similarity_metrics']['overall_similarity'],\n",
    "                reverse=True\n",
    "            )\n",
    "            \n",
    "            # Save to CSV\n",
    "            with open(filename, 'w', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([\n",
    "                    'File 1', 'File 2', 'Overall Similarity',\n",
    "                    'Token Similarity', 'Sequence Similarity',\n",
    "                    'Tree Edit Similarity', 'Subtree Hash Similarity',\n",
    "                    'Structure Similarity', 'Confidence'\n",
    "                ])\n",
    "                \n",
    "                for result in sorted_results:\n",
    "                    metrics = result['similarity_metrics']\n",
    "                    writer.writerow([\n",
    "                        os.path.basename(result['file1']),\n",
    "                        os.path.basename(result['file2']),\n",
    "                        f\"{metrics['overall_similarity']:.4f}\",\n",
    "                        f\"{metrics.get('token_similarity', 'N/A'):.4f}\" if 'token_similarity' in metrics else 'N/A',\n",
    "                        f\"{metrics.get('sequence_similarity', 'N/A'):.4f}\" if 'sequence_similarity' in metrics else 'N/A',\n",
    "                        f\"{metrics['tree_edit_similarity']:.4f}\",\n",
    "                        f\"{metrics['subtree_hash_similarity']:.4f}\",\n",
    "                        f\"{metrics['structure_similarity']:.4f}\",\n",
    "                        f\"{metrics.get('confidence', 'N/A'):.4f}\" if 'confidence' in metrics else 'N/A'\n",
    "                    ])\n",
    "            \n",
    "            messagebox.showinfo(\"Success\", f\"Results exported to {filename}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Failed to export results: {str(e)}\")\n",
    "    \n",
    "    # Button frame for directory analysis\n",
    "    dir_button_frame = ttk.Frame(dir_frame)\n",
    "    dir_button_frame.grid(row=2, column=0, columnspan=3, pady=10)\n",
    "    \n",
    "    ttk.Button(dir_button_frame, text=\"Analyze Directory\", command=analyze_directory).grid(row=0, column=0, padx=10)\n",
    "    ttk.Button(dir_button_frame, text=\"Export Results\", command=export_results).grid(row=0, column=1, padx=10)\n",
    "    \n",
    "    def update_status(message):\n",
    "        status_var.set(message)\n",
    "        if \"complete\" in message.lower() or \"error\" in message.lower():\n",
    "            progress.stop()\n",
    "    \n",
    "    # Start the GUI\n",
    "    root.mainloop()\n",
    "\n",
    "# Add this line at the end of your script to launch the GUI\n",
    "if __name__ == \"__main__\":\n",
    "    create_gui()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756cd6ca-b825-45a3-82eb-693e02a939bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
